/**:
  ros__parameters:
    node_params:
      num_cameras: 6
    interface_params:
      # autoware
      input_image_width: 1440
      input_image_height: 1080
      target_image_width: 640
      target_image_height: 384
      detection_range: [-15.0, -30.0, -2.0, 15.0, 30.0, 2.0]
      autoware_to_vad_camera_mapping: [0, 0, # FRONT
                                       4, 1, # FRONT_RIGHT
                                       2, 2, # FRONT_LEFT
                                       1, 3, # BACK
                                       3, 4, # BACK_LEFT
                                       5, 5] # BACK_RIGHT
      vad2base: [0.0, 1.0, 0.0, 0.0,
                -1.0, 0.0, 0.0, 0.0,
                 0.0, 0.0, 1.0, 0.0,
                 0.0, 0.0, 0.0, 1.0]
      # params for output
      trajectory_timestep: 1.0 # [s]
    model_params:
      # Engine paths with hierarchical structure
      image_normalization_param_mean: [103.530, 116.280, 123.675]
      image_normalization_param_std: [1.0, 1.0, 1.0]
      map_class_names: ["divider", "ped_crossing", "boundary"]
      object_class_names: ["car", "truck", "construction_vehicle", "bus", "trailer", "barrier", "motorcycle", "bicycle", "pedestrian", "traffic_cone"]
      nets:
        backbone:
          name: "backbone"
          onnx_path: "$(var model_path)/sim_vadv1.extract_img_feat.onnx"
          engine_path: "$(var model_path)/vad-tiny_backbone.engine"
          precision: "fp16"
        head:
          name: "head"
          onnx_path: "$(var model_path)/sim_vadv1_prev.pts_bbox_head.forward.onnx"
          engine_path: "$(var model_path)/vad-tiny_head.engine"
          precision: "fp32"
          inputs:
            "input_feature": "mlvl_feats.0"
            "net": "backbone"
            "name": "out.0"
        head_no_prev:
          name: "head_no_prev"
          onnx_path: "$(var model_path)/sim_vadv1.pts_bbox_head.forward.onnx"
          engine_path: "$(var model_path)/vad-tiny_head_no_prev.engine"
          precision: "fp32"
          inputs:
            "input_feature": "mlvl_feats.0"
            "net": "backbone"
            "name": "out.0"
      network_io_params:
        # BEV (Bird's Eye View) related parameters
        bev_h: 100                        # BEV grid height
        bev_w: 100                        # BEV grid width
        bev_feature_dim: 256              # Feature dimension
        
        # Image processing related parameters
        downsample_factor: 32             # Downsampling factor for image features
        
        num_decoder_layers: 3             # Number of Transformer decoder layers

        # Trajectory prediction related parameters
        prediction_num_queries: 300       # Number of detection queries (maximum objects)
        prediction_num_classes: 10        # Number of object classes
        prediction_bbox_pred_dim: 10      # 3D box prediction dimension (cx,cy,w,l,cz,h,sin,cos,vx,vy)
        prediction_trajectory_modes: 6    # Number of trajectory prediction modes per object
        prediction_timesteps: 6           # Number of prediction timesteps

        # planning
        planning_ego_commands: 3          # Number of ego vehicle control commands (right turn, left turn, straight)
        planning_timesteps: 6
        
        # Map element detection
        map_num_queries: 100              # Number of map element detection queries
        map_num_class: 3                  # Number of map element classes
        map_points_per_polylines: 20      # Number of points per polyline
              
        # can_bus
        can_bus_dim: 18                   # CAN bus data dimension
